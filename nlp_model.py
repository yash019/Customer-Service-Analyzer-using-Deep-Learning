# -*- coding: utf-8 -*-
"""sentimental_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10F4IFN0BsTAoHtVzvE3xMM8LrKdGmkvY
"""

# reason shantanu will manipulate the imdb dataset
#1 special metadata (like PAD,START,UNKNOWN,) is missing
#2 dictionary starts with 1 not 0
#<PAD> 0,<START> 1 and so on
#using lamda function-   y= lamda x:x+1(check)

from tensorflow import keras
import tensorflow as tf
import numpy as np

dataset = keras.datasets.imdb
print(dir(dataset))

(xtrain,ytrain),(xtest,ytest) = dataset.load_data(num_words=10000)

print(xtrain[0])
print(ytrain[0])

dicti = dataset.get_word_index() #throws the entire eng dictionary as per imdb
print(dicti)

print(dicti['hello']) # just checking

dicti = {k:(v+3) for (k,v) in dicti.items()}
dicti["<PAD>"]=0
dicti["<START>"]=1
dicti["<UNK>"]=2
dicti["<UNUSED"]=3

rev_dicti = {v:k for (k,v) in dicti.items()} #just to use numbers to get words as our movie reviews our in number format

#lets check
print(rev_dicti[12])

def decoder(text):
  decoded_text= [rev_dicti.get(word) for word in text]
  return decoded_text

print(decoder(xtrain[2]))

#for doing it in 1 line->
def decoder(text):
  return " ".join( [rev_dicti.get(word) for word in text])# when using join " " whatever is in here, will be in between every word

print(decoder(xtrain[0]))

#making sentences of the same length, padding with 0
xtrain_modified = keras.preprocessing.sequence.pad_sequences(xtrain,value=0,padding='post',maxlen=256)

xtest_modified = keras.preprocessing.sequence.pad_sequences(xtest,value=0,padding='post',maxlen=256)

print(decoder(xtrain_modified[2]))

print(decoder(xtest_modified[3]))

print(len(decoder(xtrain_modified[1])))

#HPYER PARAMETERS
HP_VOCABSIZE =10000
HP_EmbedVectors = 16
HP_DenseVectors = 16
HP_EPOCHS = 50

model = keras.Sequential()
model.add(keras.layers.Embedding(HP_VOCABSIZE,16))
#16 means that 10,000 input is broken into 16(taken randomly) vectors, ie 1,60,000 in total in embedding layer
#first we do input manipulation and do pattern recognition on it

model.add(keras.layers.GlobalAveragePooling1D())
#no hyperparameter for the output as there will only be 1 output

model.add(keras.layers.Dense(16,activation=tf.nn.relu))
model.add(keras.layers.Dense(1,activation=tf.nn.sigmoid))

print(model.summary())

model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')

#creating a validation data
xval= xtrain_modified [:10000]
pending_xtrain = xtrain_modified [10000:]

yval= ytrain[:10000]
pending_ytrain = ytrain[10000:]

#safe way of doing as every time we run cause of pending_train we do not lose data every time we compile

history = model.fit(pending_xtrain,pending_ytrain,epochs=HP_EPOCHS,batch_size=512,validation_data=(xval,yval))
#batch size is a hyper parameter hance random-> 256,512 etc.

model.save('mymodel.h5')
# h5 is file format for neural networks like png is for images
# this will save or export the entire model
# another way if we want weight and shape in different formats is-> using PICKLE

